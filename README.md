# Principal Component Analysis: A dimensionlity reduction algorithm

## 1) Theory - _example on paper_
### Data
Given the 2d data:
$x_1 = (3,2)$
$x_2 = (4,1)$
$x_3 = (0,-3)$
$x_4 = (-1,4)$
$x_5 = (-6,1)$
$x_6 = (-5,0)$
$x_7 = (3,1)$
$x_8 = (1,-4)$
$x_9 = (-4,2)$
$x_{10} = (1,-1)$
$x_11 = ()$
$x_12 = ()$
$x_13 = ()$
$x_14 = ()$
$x_15 = ()$

### Standardize data

$x_{std} = \frac{x - \mu_x}{\sigma_x}$

### Calculate covariance matrix
### Build $\lambda$ identity matrix and sustract covariance matrix
### Calculate determinant of previous matrix
### Solve the equation for $\lambda$ value to find eigenvalues
### Find eigenvectors
### Dimensionality projections

## 2) Implementation - _PCA from scratch_

## 3) Comparison - _Implementation with Scikit Learn_

